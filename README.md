1	Introduction

1.1	Clinical Problem and Dataset

Accurately identifying different abdominal organs in medical images is a crucial task with many applications. For example, it can help doctors diagnose diseases, plan surgeries, and monitor the health of patients. Traditionally, this identification is done manually by radiologists, which can be time-consuming and prone to human error. Developing automated systems that can classify these organs with high accuracy is pivotal. This project tackles this problem by using machine learning techniques to classify abdominal organs using a specific dataset. The data used comes from the OrganAMNIST dataset, which is a part of a larger collection of medical images. This dataset contains 58,830 two-dimensional (2D) images of various body organs. These 2D images were derived from three-dimensional (3D) Computed Tomography (CT) scans, which are a common medical imaging technique. The OrganAMNIST dataset includes images of eleven different abdominal organs, providing a diverse set of ex-amples for classification.

1.2	Techniques Used

To automatically classify the abdominal organ images, two different machine learning approaches were used: K-Nearest Neighbors (KNN) and Convolutional Neural Net-works (CNN). KNN is a relatively simple method that classifies a new image based on its similarity to previously seen images in the dataset. We manually extracted some basic features from the images, such as the average pixel intensity and how much the pixels vary, and then used these features to train the KNN model. On the other hand, CNNs are a more advanced type of machine learning model that are par-ticularly well-suited for image analysis. CNNs can automatically learn complex pat-terns and features directly from the raw pixel data of the images, without the need for manual feature extraction. Our CNN model consisted of several layers designed to identify these important visual features. We trained both the KNN and CNN models on a portion of our dataset and then tested their ability to correctly classify new, un-seen images pulled from test and validation subsets.

1.3	Result and Conclusion

After training and evaluating both models, the results showed a clear difference in their performance. The CNN significantly outperformed the KNN classifier. Specifi-cally, the CNN achieved a accuracy that was 10.36% higher than the KNN, reaching an overall accuracy of 87.36% in correctly identifying the different abdominal organs. This improvement suggests that CNNs, with their ability to automatically learn rele-vant features from image data, are a promising approach for the automated classifica-tion of abdominal organs. The higher accuracy achieved by the CNN indicates that it could be a feasible and more reliable alternative to manual classification by humans in certain settings.

2	Methods

The dataset used in this project is OrganAMNIST, a subset of the MedMNIST v2 medical image benchmark. It consists of grayscale medical images of 1x28x28 pixels, each classified into one of 11 organ classes: Bladder, Femur-left, Femur-right, Heart, Kidney-left, Kidney-right, Liver, Lung-left, Lung-right, Pancreas, and Spleen. The dataset was split into three subsets: 58,850 training images, 8,400 validation images, and 17,000 test images. The original images were rescaled to be 128x128.

2.1	K-Nearest Neighbors

The KNN method is a simple, non-parametric classification algorithm that predicts the class of a sample based on the majority class of its k-closest neighbors in the space. It does not involve any training in the traditional sense, as it simply stores the feature representations of the training set and uses them when making predictions.
To implement KNN, we manually extracted features from each image. These in-cluded the overall mean and standard deviation of pixel values, as well as the mean and standard deviation of four 64×64 image quadrants. These features were then nor-malized using the training data’s mean and standard deviation to ensure all feature dimensions had similar scales. Euclidean distance metric was used to compute pair-wise distances between validation and training samples. Finally, scikit-learn’s KNeighborsClassifier with k=3 to was implemented to classify the images. Some image augmentation methods (rotation) and morphological processing (Sobel filter and dilation) were used to optimize the performance of KNN.

2.2	Convolutional Neural Network

CNNs are deep learning models especially suited for image classification tasks. They work by automatically learning spatial features (like edges or corners) from raw pixel data through layers of convolutions, non-linear activations, and pooling.
    Our CNN architecture consists of five convolutional layers with increasing channel depth and batch normalization, followed by ReLU activations. Max pooling is ap-plied after certain layers to reduce spatial dimensions. The convolutional blocks are followed by a fully connected classification head with two hidden layers before out-putting logits for 11 classes.  The model was trained on a CPU (GPU was unavaila-ble) with a batch size of 128, the Adam optimizer or Stochastic Gradient Descent (SGD) optimizer, and CrossEntropy loss. The images were normalized to have a mean of 0.5 and standard deviation of 0.5 before being passed to the model. Addi-tionally, processing like image rotation were used to improve generalization and pre-vent overfitting. Training and evaluation loops were implemented with accuracy and loss tracking on both the training and validation datasets.

2.3	Metrics

To evaluate the performance of the KNN and CNN models, several key metrics are used to assess how well the methods perform. In the case of the KNN model, classifi-cation performance is measured using accuracy and the classification report, which includes precision, recall, and F1-score for each class. Accuracy is calculated by com-paring the predicted labels to the true labels for the validation dataset, and the classi-fication report provides a deeper analysis of how well the model performs in terms of correctly identifying instances of each class. For the CNN model, the evaluation is based on the training and test losses, as well as the accuracy per epoch. Accuracy is computed as the percentage of correctly classified instances from the total number of instances in the dataset. The best model is determined by selecting the model with the lowest test loss, and the training process also tracks the total training time for performance benchmarking. These metrics provide a understanding of the models' effectiveness in handling classifying abdominal organs.

3	Results

3.1	KNN results
 
The bar chart compares precision, recall, and F1-scores for each class between the Base KNN model and an Optimized KNN model that uses image processing and augmentation. Precision is the proportion of predicted positives that are actually cor-rect, recall is the proportion of actual positives that are correctly identified, and F1-score balances precision and recall. Across most classes, the optimized model shows noticeable improvements in at least one metric, with particularly strong gains in clas-ses where the base model struggled. For example, Pancreas, Spleen, and Kidney-left saw significant improvements in recall and F1-score under the optimized model, indi-cating better detection of underrepresented or harder-to-classify categories. Similarly, Liver, Lung-left, and Lung-right perform near-perfectly across all metrics with both models, showing these classes are easily distinguishable. Classes like Heart and Kid-ney-left still show relatively low recall and F1-scores, suggesting that even with op-timization, these remain challenging for KNN to classify. Overall, the optimized KNN clearly benefits from preprocessing, with increased precision across classes.

3.2	Method 2 results
 
The plot compares the test accuracy of four different CNN models for organ classifica-tion across 10 training epochs. The models vary by optimization algorithm (SGD or Adam) and whether data augmentation was used. The CNN with Adam + Augmenta-tion consistently performs best, achieving the highest final accuracy of 87.36%. Ad-am consistently outperforms SGD regardless of augmentation, with SGD showing a significant performance drop in epoch 2 (down to about 35% accuracy) before recover-ing. All models eventually converge to relatively stable performance by epochs 5-6, with final accuracies ranging from 82.61% (SGD without augmentation) to 87.36% (Adam with augmentation). The table below the graph also shows that models using Adam required more training time than their SGD counterparts, with augmentation further increasing training time across both optimizers. This suggests that the accura-cy improvements from Adam and augmentation come at the cost of longer training times.
 
The plot on the left shows that the best CNN model, which used the Adam optimizer and data augmentation, significantly outperforms the best KNN model, which incor-porated image processing. The CNN achieves a test accuracy of 87.36%, which is notably higher than the KNN's accuracy of 77.00%. The plot on the right proves that CNN's test accuracy consistently surpasses the best accuracy achieved by the KNN model, even despite epoch dips. This reinforces the conclusion that the CNN is a more effective model for this classification task.

4	Conclusions

4.1	Overall

Based on the results presented, the Convolutional Neural Network (CNN) with Adam optimization and data augmentation emerged as the superior method for abdominal organ classification in this study. This model achieved the highest test accuracy of 87.36%, significantly outperforming the best KNN model, which showed lower accu-racy even after optimization. The CNN's strength lies in its ability to automatically learn complex hierarchical features directly from the raw pixel data. Unlike the KNN, which relied on manually engineered features (overall mean, standard deviation, and quadrant statistics), the convolutional layers in the CNN can identify intricate spatial patterns and textures that are crucial for distinguishing between different abdominal organs. The use of Adam optimization likely contributed to faster and more stable convergence during training compared to SGD. Furthermore, the application of data augmentation tech-niques helped to improve the model's generalization by exposing it to a wider variety of image transformations. With more time and computing capacity, several additional experiments could be conducted to improve classification performance. Firstly, exploring deeper and more complex CNN architectures could potentially capture even more intricate features and lead to higher accuracy. Secondly, experimenting with different data augmentation strategies, including more techniques like shifting, scaling, sharpening, and filtering, might further improve the model's robustness. Thirdly, given access to a GPU, train-ing the CNN models for a larger number of epochs and experimenting with other parameter tuning (e.g., learning rate and batch size) could lead to further performance gains. The resources for this experiment were not sufficient to run a complex, robust algorithm.

4.2	Limitations

This study has several limitations that should be considered when interpreting the results. Firstly, the dataset used, OrganAMNIST, is a subset of MedMNIST v2 and consists of 2D images derived from 3D CT scans. While this provides a large number of samples, the classification is performed on individual 2D slices, which may not fully capture the 3D spatial context of the organs. Utilizing the full 3D CT volumes could potentially provide better information and improve classification accuracy. Sec-ondly, the computational resources available were limited, as the CNN models were trained on a CPU. This constrained the complexity of the models that could be ex-plored and the duration of the training process. Access to GPUs would likely enable the training of larger and more sophisticated models for longer durations, potentially leading to better performance. Thirdly, the KNN classifier relied on a relatively sim-ple set of manual features. While some optimization was attempted, these features may not be as effective as the features learned automatically by the CNN. Finally, the evaluation metrics primarily focused on overall accuracy and class-specific precision, recall, and F1-scores. While these are standard metrics for classification tasks, further analysis could include evaluating the models' performance on more challenging or ambiguous cases and visualizing the learned features of the CNN to gain a better un-derstanding of its decision-making process. 

